Notes by Allen Cee



## Lecture 01 机器学习简介



### 机器学习算法 Machine learning algorithms

**常见算法**

1. Supervised learning 监督学习
2. Unsupervised learning 无监督学习

**其他**

1. Reinforcement learning 强化学习
2. Recommender systems 推荐系统

### 监督学习 Supervised Learning

1. 监督学习：已知部分数据集，给出算法，预测新数据
   1. 回归问题 Regression Problem：监督学习的一种，预测连续值 continuous values（如房价、股价）[Fig 01-01]
   2. 分类问题 Classification Problem：监督学习的一种，预测离散值输出 discrete valued ouput（如肿瘤性质） [Fig 01-02] & [Fig 01-03]

* 学习算法能够处理无穷多的属性（支持向量机 Support Vector）

### 无监督学习 Unsupervised Learning

1. 无监督学习：数据集中所有数据是一样的、没有属性，通过算法，找到某种结构（如谷歌新闻分类、特定基因判定）【个人感觉和主成分分析、因子正则化比较像】
   1. 聚类 Cluster：无监督学习的一种，具体定义还不清楚，将无属性数据通过算法分出不同属性的类（如社交网络分析、市场分割、星体分类等）[Fig 01-04]

* 鸡尾酒宴会问题 Cocktail Party Problem：不同音源如何区分；有鸡尾酒算法
* Octave常用来开发程序原型，因为内置了很多学习算法，如`svd()`奇艺值分解



## Lecture 02 模型基础



### 模型表示 Model Representation

**Eg 02-01: Housing Prices** [Fig 02-01]

* 训练集 Training Set：监督学习中已知的数据集 [Fig 02-02]

**机器学习中常见表示** [Fig 02-03]

### 成本函数 Cost Function

* 确定模型参数的依据是「使假设函数预测的$h(x)$与$y$尽可能接近」，所用的函数称为成本函数 $J(\theta_0, \theta_1)$ [Fig 02-04]

* 通常用平方误差代价函数Squared Error Function，其他函数也可，平方误差代价函数对大多数回归问题效果都不错
* 区分假设函数和代价函数：一个是$x\to y$的映射，一个是$\theta \to J$的映射；图为1个参数代价函数选取最佳参数值的直观展示 [Fig 02-05] & [Fig 02-06] & [Fig 02-07] & [Fig 02-08]
* 两个参数的代价函数会形成弓形曲面，竖向值代表代价函数的值，投影到地面形成轮廓图Contour Plot/Contour Figure【类似等高线图】[Fig 02-10]

### 梯度下降算法 Gradient Descent

* 用于使代价函数$J$最小，也用于机器学习其他领域，非常常用

**梯度下降算法的思想** [Fig 02-11]

1. 选择代价函数$J$中$\theta$的初始值，一般都设为$0$
2. 不断改变$\theta$的值，直至代价函数最小

* 就像下山一样，像下降最快的方向前进，通过偏微分判断$\theta_i$的改变方向 [Fig 02-12]
* 梯度下降算法可能出现多个局部最优解，而初始值可能非常接近
* $:=$是赋值符号 [Fig 02-13]
* $\alpha$是对应的学习速率，即$\theta_i$下降的步长
* 注意：需要同时更新$\theta_0$和$\theta_1$的值
* $\alpha$过大可能导致结果无法收敛，甚至发散 [Fig 02-14]
* 因为导数会越来越小，所以$\alpha$保持不变也会使结果收敛（步长趋近于$0$，达到局部最优点）

### 线性回归 Linear Regression

**线性回归代价函数的偏微分求解** [Fig 02-15] & [Fig 02-16]

* 梯度下降算法对应的代价函数需要是弓形曲线/曲面，即凸函数Convex Function
* 批处理梯度下降算法 Batch Gradient Descent：一次性用所有训练集中的样本计算最佳参数，即$\sum$的范围是$1$到$m$【意思应该是有时候可能处理训练集的子集】



## Lecture 03 线性代数



### 矩阵和向量 Matrices and Vectors

**矩阵的行、列、维度** [Fig 03-01]

**矩阵的元素：下标先行后列** [Fig 03-02]

**向量：一列矩阵** [Fig 03-03]

* 数学（线性代数中）索引通常从1开始，机器学习中通常从0开始，具体情况具体讨论
* 大写字母通常表示矩阵，小写字母表示数字

### 矩阵与标量的乘法

* 维度相同才能加减

### 矩阵与向量的乘法

* 向量的维度和矩阵的列数相同，向量由上至下和矩阵由左至右相乘 [Fig 03-04] & [Fig 03-05]
* 原始数据和假设函数一起计算预测值时，用矩阵乘向量更简单、更快 [Fig 03-06]

### 矩阵与矩阵的乘法

* 分别用第一个矩阵乘第二个矩阵按列拆分的向量，再合并 [Fig 03-07] & [Fig 03-08]
* 原始数据和多个假设函数一起计算预测值时，可以用矩阵乘矩阵【为调整参数比较收益提供了思路】 [Fig 03-09]

### 矩阵乘法的性质

* 不具备交换律 not commutative
* 单位矩阵 Identity Matrix：用$I$表示，行列相同，左上角到右下角是1，其余是0；满足$A\times I=I\times A=A$【注意两个$I$不是同一个，前者是$A$的列数，后者是$A$的行数】 [Fig 03-10]

### 逆运算和转置 Inverse and Transpose

* 逆矩阵的概念对应实数的倒数
* 只有行列相同的矩阵才可能有逆矩阵，即方阵；但不是所有方阵都有逆矩阵；0没有倒数，显然所有元素是零的矩阵没有逆矩阵；逆矩阵不存在的矩阵称为奇异矩阵 singular 或退化矩阵 degenerate
* 直观上可以将没有逆矩阵的矩阵想象为非常接近于0
* $A$的转置矩阵$A^T$是将原来的行从上至下依次变为从左至右的列，即$A_{ij}=A^T_{ji}$ [Fig 03-11] 



## Lecture 04 线性回归

